# M4 Max 최적화 학습 설정 (Large 모델용)

seed: 1337
device: mps  # Metal Performance Shaders

data:
  train_path: data/processed/train.txt
  valid_path: data/processed/valid.txt
  tokenizer_model: models/tokenizer.model

train:
  # 배치 설정 (메모리 제약)
  batch_size: 8
  gradient_accumulation_steps: 8
  effective_batch_size: 64  # 8 * 8
  max_seq_length: 1536  # 2048보다 약간 작게
  
  # 옵티마이저
  learning_rate: 1.5e-4  # 더 큰 모델은 더 낮은 LR
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  # 스케줄러
  warmup_steps: 5000
  max_steps: 200000
  lr_scheduler: cosine
  min_lr: 1.5e-5  # learning_rate의 10%
  
  # 체크포인트
  save_steps: 5000
  eval_steps: 1000
  logging_steps: 100
  output_dir: checkpoints/large/
  
  # 최적화
  mixed_precision: true  # FP16
  gradient_checkpointing: true  # 메모리 절약 필요
  compile_model: true  # torch.compile 사용
  
  # 데이터 로딩
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4

optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

# W&B 로깅 (선택)
wandb:
  enabled: false  # true로 변경하여 활성화
  project: kr-mini-llm
  name: large-m4max
  tags: [m4max, large, 800m]

# 예상 성능
# - 학습 속도: 1-1.5 steps/sec
# - 총 학습 시간: ~44시간
# - 메모리 사용: ~20GB

# Made with Bob
