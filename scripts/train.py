from __future__ import annotations

"""
í•™ìŠµ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸ (M4 Max ìµœì í™”).

M4 Maxì˜ Metal Performance Shaders (MPS)ë¥¼ í™œìš©í•œ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸.
Phase 4ì—ì„œ ë³¸ê²© í•™ìŠµ ë£¨í”„/ì²´í¬í¬ì¸íŠ¸/ë¡œê¹…ì„ êµ¬í˜„í•©ë‹ˆë‹¤.
"""

import argparse
import sys
from pathlib import Path
from typing import Optional

import torch
import yaml


def setup_device(device_name: str = "auto") -> torch.device:
    """
    í•™ìŠµì— ì‚¬ìš©í•  ë””ë°”ì´ìŠ¤ ì„¤ì •
    
    Args:
        device_name: "auto", "mps", "cuda", "cpu" ì¤‘ í•˜ë‚˜
        
    Returns:
        torch.device ê°ì²´
    """
    if device_name == "auto":
        # ìë™ ê°ì§€: MPS > CUDA > CPU
        if torch.backends.mps.is_available():
            device = torch.device("mps")
            print("ğŸš€ M4 Max Metal Performance Shaders (MPS) ì‚¬ìš©")
        elif torch.cuda.is_available():
            device = torch.device("cuda")
            print(f"ğŸš€ CUDA ì‚¬ìš© (GPU: {torch.cuda.get_device_name(0)})")
        else:
            device = torch.device("cpu")
            print("âš ï¸  CPU ì‚¬ìš© (í•™ìŠµ ì†ë„ê°€ ëŠë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    else:
        device = torch.device(device_name)
        print(f"ğŸš€ ë””ë°”ì´ìŠ¤: {device}")
    
    # MPS ë©”ëª¨ë¦¬ ìµœì í™”
    if device.type == "mps":
        try:
            # MPS ë©”ëª¨ë¦¬ì˜ 80%ê¹Œì§€ ì‚¬ìš© (ì‹œìŠ¤í…œ ì•ˆì •ì„± ìœ ì§€)
            torch.mps.set_per_process_memory_fraction(0.8)
            print("   MPS ë©”ëª¨ë¦¬ ì œí•œ: 80%")
        except Exception as e:
            print(f"   MPS ë©”ëª¨ë¦¬ ì„¤ì • ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
    
    return device


def check_pytorch_version() -> None:
    """PyTorch ë²„ì „ í™•ì¸ ë° ê²½ê³ """
    version = torch.__version__
    major, minor = map(int, version.split('.')[:2])
    
    print(f"ğŸ“¦ PyTorch ë²„ì „: {version}")
    
    if major < 2:
        print("âš ï¸  ê²½ê³ : PyTorch 2.0+ ê¶Œì¥ (torch.compile ì§€ì›)")
    
    if torch.backends.mps.is_available():
        print("âœ… MPS ë°±ì—”ë“œ ì‚¬ìš© ê°€ëŠ¥")
        if major == 2 and minor < 1:
            print("âš ï¸  ê²½ê³ : PyTorch 2.1+ ê¶Œì¥ (MPS ì•ˆì •ì„± í–¥ìƒ)")
    else:
        print("âŒ MPS ë°±ì—”ë“œ ì‚¬ìš© ë¶ˆê°€")


def load_config(config_path: str) -> dict:
    """YAML ì„¤ì • íŒŒì¼ ë¡œë“œ"""
    path = Path(config_path)
    if not path.exists():
        raise FileNotFoundError(f"ì„¤ì • íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {config_path}")
    
    with open(path, 'r', encoding='utf-8') as f:
        return yaml.safe_load(f)


def validate_config(cfg: dict) -> None:
    """ì„¤ì • ê²€ì¦"""
    required_keys = ['seed', 'device', 'data', 'train']
    for key in required_keys:
        if key not in cfg:
            raise ValueError(f"í•„ìˆ˜ ì„¤ì • í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: {key}")
    
    # í•™ìŠµ ì„¤ì • ê²€ì¦
    train_cfg = cfg['train']
    required_train_keys = ['batch_size', 'learning_rate', 'max_steps']
    for key in required_train_keys:
        if key not in train_cfg:
            raise ValueError(f"í•„ìˆ˜ í•™ìŠµ ì„¤ì • í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤: train.{key}")


def print_training_info(cfg: dict, model_cfg: dict, device: torch.device) -> None:
    """í•™ìŠµ ì •ë³´ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š í•™ìŠµ ì„¤ì • ì •ë³´")
    print("="*60)
    
    # ëª¨ë¸ ì •ë³´
    print(f"\nğŸ¤– ëª¨ë¸:")
    print(f"   - Hidden size: {model_cfg.get('hidden_size', 'N/A')}")
    print(f"   - Layers: {model_cfg.get('num_layers', 'N/A')}")
    print(f"   - Heads: {model_cfg.get('num_heads', 'N/A')}")
    print(f"   - Vocab size: {model_cfg.get('vocab_size', 'N/A')}")
    
    # í•™ìŠµ ì„¤ì •
    train_cfg = cfg['train']
    print(f"\nğŸ‹ï¸  í•™ìŠµ:")
    print(f"   - Batch size: {train_cfg.get('batch_size', 'N/A')}")
    print(f"   - Gradient accumulation: {train_cfg.get('gradient_accumulation_steps', 'N/A')}")
    effective_batch = train_cfg.get('batch_size', 0) * train_cfg.get('gradient_accumulation_steps', 1)
    print(f"   - Effective batch size: {effective_batch}")
    print(f"   - Learning rate: {train_cfg.get('learning_rate', 'N/A')}")
    print(f"   - Max steps: {train_cfg.get('max_steps', 'N/A'):,}")
    print(f"   - Mixed precision: {train_cfg.get('mixed_precision', False)}")
    print(f"   - Gradient checkpointing: {train_cfg.get('gradient_checkpointing', False)}")
    print(f"   - Compile model: {train_cfg.get('compile_model', False)}")
    
    # ë””ë°”ì´ìŠ¤ ì •ë³´
    print(f"\nğŸ’» ë””ë°”ì´ìŠ¤:")
    print(f"   - Device: {device}")
    if device.type == "mps":
        print(f"   - MPS ìµœì í™”: í™œì„±í™”")
    
    # ë°ì´í„° ì •ë³´
    data_cfg = cfg.get('data', {})
    print(f"\nğŸ“ ë°ì´í„°:")
    print(f"   - Train: {data_cfg.get('train_path', 'N/A')}")
    print(f"   - Valid: {data_cfg.get('valid_path', 'N/A')}")
    print(f"   - Tokenizer: {data_cfg.get('tokenizer_model', 'N/A')}")
    
    print("\n" + "="*60 + "\n")


def main() -> None:
    ap = argparse.ArgumentParser(
        description="kr-mini-llm í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ (M4 Max ìµœì í™”)"
    )
    ap.add_argument(
        "--config",
        type=str,
        default="configs/training_m4max.yaml",
        help="í•™ìŠµ ì„¤ì • íŒŒì¼ (YAML)"
    )
    ap.add_argument(
        "--model_config",
        type=str,
        default="configs/model_medium.yaml",
        help="ëª¨ë¸ ì„¤ì • íŒŒì¼ (YAML)"
    )
    ap.add_argument(
        "--output_dir",
        type=str,
        default=None,
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ì„¤ì • íŒŒì¼ì˜ ê°’ì„ ì˜¤ë²„ë¼ì´ë“œ)"
    )
    ap.add_argument(
        "--resume_from",
        type=str,
        default=None,
        help="ì²´í¬í¬ì¸íŠ¸ì—ì„œ í•™ìŠµ ì¬ê°œ"
    )
    args = ap.parse_args()

    # PyTorch ë²„ì „ í™•ì¸
    check_pytorch_version()
    
    # ì„¤ì • ë¡œë“œ
    print(f"\nğŸ“„ ì„¤ì • íŒŒì¼ ë¡œë“œ ì¤‘...")
    try:
        cfg = load_config(args.config)
        model_cfg = load_config(args.model_config)
        print(f"   âœ… í•™ìŠµ ì„¤ì •: {args.config}")
        print(f"   âœ… ëª¨ë¸ ì„¤ì •: {args.model_config}")
    except Exception as e:
        print(f"   âŒ ì„¤ì • ë¡œë“œ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ì„¤ì • ê²€ì¦
    try:
        validate_config(cfg)
    except ValueError as e:
        print(f"âŒ ì„¤ì • ê²€ì¦ ì‹¤íŒ¨: {e}")
        sys.exit(1)
    
    # ë””ë°”ì´ìŠ¤ ì„¤ì •
    device = setup_device(cfg.get('device', 'auto'))
    
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •
    if args.output_dir:
        output_dir = Path(args.output_dir)
    else:
        output_dir = Path(cfg['train'].get('output_dir', 'checkpoints/'))
    
    output_dir.mkdir(parents=True, exist_ok=True)
    print(f"ğŸ“‚ ì¶œë ¥ ë””ë ‰í† ë¦¬: {output_dir}")
    
    # í•™ìŠµ ì •ë³´ ì¶œë ¥
    print_training_info(cfg, model_cfg, device)
    
    # Seed ì„¤ì •
    seed = cfg.get('seed', 1337)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    print(f"ğŸ² Random seed: {seed}")
    
    print("\n" + "="*60)
    print("âš ï¸  Phase 2-4 êµ¬í˜„ í•„ìš”")
    print("="*60)
    print("ë‹¤ìŒ ë‹¨ê³„:")
    print("1. src/model/* - ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„ (RoPE, GQA, SwiGLU)")
    print("2. src/data/* - ë°ì´í„° íŒŒì´í”„ë¼ì¸ êµ¬í˜„")
    print("3. src/training/* - í•™ìŠµ ë£¨í”„ êµ¬í˜„")
    print("4. ì´ ìŠ¤í¬ë¦½íŠ¸ì—ì„œ í•™ìŠµ ë£¨í”„ ì—°ê²°")
    print("="*60 + "\n")
    
    # ì„¤ì • ì €ì¥
    config_save_path = output_dir / "config.yaml"
    with open(config_save_path, 'w', encoding='utf-8') as f:
        yaml.dump(cfg, f, default_flow_style=False, allow_unicode=True)
    print(f"ğŸ’¾ ì„¤ì • ì €ì¥: {config_save_path}")
    
    model_config_save_path = output_dir / "model_config.yaml"
    with open(model_config_save_path, 'w', encoding='utf-8') as f:
        yaml.dump(model_cfg, f, default_flow_style=False, allow_unicode=True)
    print(f"ğŸ’¾ ëª¨ë¸ ì„¤ì • ì €ì¥: {model_config_save_path}")


if __name__ == "__main__":
    main()


