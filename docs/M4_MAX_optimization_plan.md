# M4 Max μµμ ν™” κ³„ν

## π–¥οΈ ν•λ“μ›¨μ–΄ μ¤ν™
- **Chip**: Apple M4 Max
- **CPU**: 14-core (10 Performance + 4 Efficiency)
- **GPU**: 32-core
- **ν†µν•© λ©”λ¨λ¦¬**: 36GB
- **λ©”λ¨λ¦¬ λ€μ—­ν­**: ~400GB/s (μμƒ)

## π― μµμ ν™”λ λ¨λΈ ν¬κΈ° μ μ•

### κ¶μ¥: Medium λ¨λΈ (300M-500M νλΌλ―Έν„°)
36GB λ©”λ¨λ¦¬λ΅ μ•μ •μ μΌλ΅ ν•™μµ κ°€λ¥ν• ν¬κΈ°

#### λ¨λΈ μ„¤μ • (Medium)
```yaml
# configs/model_medium.yaml
vocab_size: 32000
hidden_size: 1024          # 768 β†’ 1024 (33% μ¦κ°€)
num_layers: 24             # 12 β†’ 24 (2λ°°)
num_heads: 16              # 12 β†’ 16
num_kv_heads: 4            # GQA μ μ§€
intermediate_size: 4096    # 2048 β†’ 4096 (2λ°°, SwiGLU)
max_seq_length: 2048       # 1024 β†’ 2048 (2λ°°)
rope_theta: 10000.0
dropout: 0.1

# μμƒ νλΌλ―Έν„°: ~350M
```

#### ν•™μµ μ„¤μ • (Medium)
```yaml
# configs/training_m4max.yaml
seed: 1337
device: mps  # M4 Max Metal Performance Shaders

data:
  train_path: data/processed/train.txt
  valid_path: data/processed/valid.txt
  tokenizer_model: models/tokenizer.model

train:
  batch_size: 16           # 4 β†’ 16 (4λ°°)
  gradient_accumulation_steps: 4  # 8 β†’ 4
  effective_batch_size: 64  # 16 * 4 = 64
  max_seq_length: 2048     # 512 β†’ 2048 (4λ°°)
  learning_rate: 2.0e-4    # 3e-4 β†’ 2e-4 (ν° λ¨λΈμ€ λ‚®μ€ LR)
  warmup_steps: 4000       # 2000 β†’ 4000
  max_steps: 200000        # 100k β†’ 200k (λ” κΈ΄ ν•™μµ)
  save_steps: 5000
  eval_steps: 1000
  mixed_precision: true    # FP16
  gradient_checkpointing: false  # λ©”λ¨λ¦¬ μ¶©λ¶„ν•λ©΄ λ„κΈ° (μ†λ„ ν–¥μƒ)
  max_grad_norm: 1.0
  weight_decay: 0.1
  output_dir: checkpoints/medium/

optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

scheduler:
  type: cosine
  min_lr: 2.0e-5  # learning_rateμ 10%
```

### μ„ νƒ: Large λ¨λΈ (700M-1B νλΌλ―Έν„°)
λ©”λ¨λ¦¬λ¥Ό μµλ€ν• ν™μ©ν•λ” κ³µκ²©μ  μ„¤μ •

#### λ¨λΈ μ„¤μ • (Large)
```yaml
# configs/model_large.yaml
vocab_size: 32000
hidden_size: 1536          # 1.5λ°° μ¦κ°€
num_layers: 24
num_heads: 24
num_kv_heads: 6            # GQA
intermediate_size: 6144    # 4 * hidden_size
max_seq_length: 2048
rope_theta: 10000.0
dropout: 0.1

# μμƒ νλΌλ―Έν„°: ~800M
```

#### ν•™μµ μ„¤μ • (Large)
```yaml
# configs/training_m4max_large.yaml
train:
  batch_size: 8            # λ©”λ¨λ¦¬ μ μ•½
  gradient_accumulation_steps: 8
  effective_batch_size: 64
  max_seq_length: 1536     # 2048λ³΄λ‹¤ μ•½κ°„ μ‘κ²
  gradient_checkpointing: true  # λ©”λ¨λ¦¬ μ μ•½ ν•„μ”
```

## π“ λ©”λ¨λ¦¬ μ‚¬μ©λ‰ μ¶”μ •

### Medium λ¨λΈ (350M)
```
λ¨λΈ νλΌλ―Έν„°: 350M * 4 bytes (FP32) = 1.4GB
  β†’ FP16 ν•™μµ: 350M * 2 bytes = 0.7GB

μµν‹°λ§μ΄μ € μƒνƒ (AdamW):
  - νλΌλ―Έν„° λ³µμ‚¬λ³Έ: 0.7GB
  - Momentum: 0.7GB
  - Variance: 0.7GB
  β†’ μ΄ 2.1GB

Gradient: 0.7GB

λ°°μΉ λ°μ΄ν„° (batch=16, seq=2048):
  - Activations: ~4-6GB (gradient checkpointing μ—†μ΄)
  - μ…λ ¥ λ°μ΄ν„°: ~0.5GB

μ΄ μμƒ: ~8-10GB (ν•™μµ μ‹)
β†’ 36GB λ©”λ¨λ¦¬μ—μ„ μ—¬μ λ΅­κ² μ‹¤ν–‰ κ°€λ¥
```

### Large λ¨λΈ (800M)
```
λ¨λΈ νλΌλ―Έν„°: 800M * 2 bytes (FP16) = 1.6GB
μµν‹°λ§μ΄μ € μƒνƒ: ~4.8GB
Gradient: 1.6GB
λ°°μΉ λ°μ΄ν„°: ~8-12GB (gradient checkpointing μ‚¬μ©)

μ΄ μμƒ: ~18-22GB
β†’ 36GB λ©”λ¨λ¦¬μ—μ„ μ‹¤ν–‰ κ°€λ¥ν•λ‚ μ—¬μ  μ μ
```

## π€ μ„±λ¥ μµμ ν™” μ „λµ

### 1. Metal Performance Shaders (MPS) ν™μ©
```python
# PyTorchμ—μ„ MPS μ‚¬μ©
device = torch.device("mps")
model = model.to(device)

# MPS μµμ ν™” ν
torch.mps.set_per_process_memory_fraction(0.8)  # λ©”λ¨λ¦¬ 80% μ‚¬μ©
```

### 2. λ°μ΄ν„° λ΅λ”© μµμ ν™”
```yaml
dataloader:
  num_workers: 8           # CPU μ½”μ–΄ ν™μ© (14μ½”μ–΄ μ¤‘ 8κ°)
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4       # λ―Έλ¦¬ λ΅λ“
```

### 3. μ»΄νμΌ μµμ ν™” (PyTorch 2.0+)
```python
# torch.compileλ΅ μ†λ„ ν–¥μƒ
model = torch.compile(model, mode="reduce-overhead")
```

### 4. Flash Attention (μ„ νƒ)
```python
# xformers λλ” flash-attention μ‚¬μ©
# MPSμ—μ„ μ§€μ› μ—¬λ¶€ ν™•μΈ ν•„μ”
from xformers.ops import memory_efficient_attention
```

## π“ μμƒ ν•™μµ μ‹κ°„

### Medium λ¨λΈ (350M)
- **Steps**: 200,000
- **Effective batch size**: 64
- **Tokens per step**: 64 * 2048 = 131,072
- **Total tokens**: 200k * 131k β‰ 26B tokens

**μμƒ μ†λ„** (M4 Max 32-core GPU):
- ~2-3 steps/sec (FP16, no gradient checkpointing)
- **μ΄ ν•™μµ μ‹κ°„**: 200k / 2.5 β‰ 80,000μ΄ β‰ **22μ‹κ°„**

### Large λ¨λΈ (800M)
- **μμƒ μ†λ„**: ~1-1.5 steps/sec
- **μ΄ ν•™μµ μ‹κ°„**: 200k / 1.25 β‰ 160,000μ΄ β‰ **44μ‹κ°„**

## π― λ°μ΄ν„° μ”κµ¬μ‚¬ν•­

### κ¶μ¥ λ°μ΄ν„° ν¬κΈ°
- **Minimum**: 10GB μ›λ³Έ ν…μ¤νΈ (ν† ν°ν™” ν›„ ~30B tokens)
- **Optimal**: 20-50GB μ›λ³Έ ν…μ¤νΈ (ν† ν°ν™” ν›„ ~60-150B tokens)

### λ°μ΄ν„° μ†μ¤ (ν•κµ­μ–΄)
1. **AI Hub** (μ°μ„ μμ„ μµμƒ)
   - μΌμƒλ€ν™” λ°μ΄ν„°
   - λ¬Έμ„μ”μ•½ λ°μ΄ν„°
   - λ‰΄μ¤ κΈ°μ‚¬

2. **Korean Wikipedia** (10GB+)
   - μ„ν‚¤λ―Έλ””μ–΄ λ¤ν”„

3. **λ‚λ¬΄μ„ν‚¤** (20GB+)
   - ν¬λ΅¤λ§ ν›„ μ „μ²λ¦¬

4. **λ¨λ‘μ λ§λ­‰μΉ** (κµ­λ¦½κµ­μ–΄μ›)
   - μ‹ λ¬Έ, λ¬Έμ–΄μ²΄, κµ¬μ–΄μ²΄

5. **Common Crawl Korean**
   - μ›Ή ν¬λ΅¤λ§ λ°μ΄ν„°

## π”§ μ¶”κ°€ μµμ ν™” μµμ…

### 1. Gradient Accumulation λ™μ  μ΅°μ •
```python
# λ©”λ¨λ¦¬ μ‚¬μ©λ‰μ— λ”°λΌ λ™μ  μ΅°μ •
if memory_usage > 0.8:
    gradient_accumulation_steps *= 2
    batch_size //= 2
```

### 2. Mixed Precision κ³ κΈ‰ μ„¤μ •
```python
from torch.cuda.amp import GradScaler, autocast

scaler = GradScaler()
# Loss scalingμΌλ΅ FP16 μ•μ •μ„± ν–¥μƒ
```

### 3. Checkpoint Averaging
```python
# λ§μ§€λ§‰ Nκ° μ²΄ν¬ν¬μΈνΈ ν‰κ· μΌλ΅ μ„±λ¥ ν–¥μƒ
# Stochastic Weight Averaging (SWA)
```

## π“ κµ¬ν„ μ°μ„ μμ„

### Phase 2A: λ¨λΈ μ•„ν‚¤ν…μ² (3-4μΌ)
1. RoPE κµ¬ν„
2. RMSNorm κµ¬ν„
3. GQA κµ¬ν„
4. SwiGLU FFN κµ¬ν„
5. TransformerBlock ν†µν•©
6. μ „μ²΄ λ¨λΈ ν†µν•©

### Phase 2B: ν…μ¤νΈ λ° κ²€μ¦ (1-2μΌ)
1. λ‹¨μ„ ν…μ¤νΈ
2. Forward/Backward pass κ²€μ¦
3. λ©”λ¨λ¦¬ ν”„λ΅νμΌλ§
4. λ”λ―Έ λ°μ΄ν„°λ΅ overfitting ν…μ¤νΈ

### Phase 3: λ°μ΄ν„° νμ΄ν”„λΌμΈ (3-5μΌ)
1. λ°μ΄ν„° μμ§‘ (10-20GB)
2. μ „μ²λ¦¬ νμ΄ν”„λΌμΈ
3. Tokenizer ν•™μµ (32k vocab)
4. Dataset ν΄λμ¤ κµ¬ν„
5. DataLoader μµμ ν™”

### Phase 4: ν•™μµ (2-3μ£Ό)
1. Trainer κµ¬ν„
2. Sanity check (μ‘μ€ λ°μ΄ν„°)
3. λ³Έκ²© ν•™μµ (200k steps)
4. λ¨λ‹ν„°λ§ λ° μ΅°μ •

## π― μ„±κ³µ κΈ°μ¤€

### Medium λ¨λΈ
- **Validation Perplexity**: < 15
- **μƒμ„± ν’μ§**: λ¬Έλ²•μ μΌλ΅ μ¬λ°”λ¥Έ ν•κµ­μ–΄
- **μ¶”λ΅  μ†λ„**: > 50 tokens/sec
- **ν•™μµ μ‹κ°„**: < 30μ‹κ°„

### Large λ¨λΈ
- **Validation Perplexity**: < 12
- **μƒμ„± ν’μ§**: λ¬Έλ§¥ μΌκ΄€μ„± + μ°½μμ„±
- **μ¶”λ΅  μ†λ„**: > 30 tokens/sec
- **ν•™μµ μ‹κ°„**: < 50μ‹κ°„

## π¨ μ£Όμμ‚¬ν•­

### M4 Max νΉν™” κ³ λ ¤μ‚¬ν•­
1. **μ—΄ κ΄€λ¦¬**: μ¥μ‹κ°„ ν•™μµ μ‹ μΏ¨λ§ ν¨λ“ κ¶μ¥
2. **μ „μ› μ—°κ²°**: κ³ μ„±λ¥ λ¨λ“ μ μ§€
3. **λ°±κ·ΈλΌμ΄λ“ μ•±**: ν•™μµ μ¤‘ μµμ†ν™”
4. **MPS μ•μ •μ„±**: PyTorch 2.1+ μ‚¬μ© κ¶μ¥

### λ©”λ¨λ¦¬ κ΄€λ¦¬
- 36GB μ¤‘ ~30GBκΉμ§€ μ‚¬μ© κ°€λ¥
- μ‹μ¤ν… μμ•½ λ©”λ¨λ¦¬ κ³ λ ¤
- OOM λ°μƒ μ‹ batch size κ°μ†

## π“ μ°Έκ³  μλ£

### M4 Max μµμ ν™”
- Apple Metal Performance Shaders λ¬Έμ„
- PyTorch MPS Backend κ°€μ΄λ“

### λ¨λΈ μ•„ν‚¤ν…μ²
- LLaMA 2 λ…Όλ¬Έ λ° κµ¬ν„
- Mistral 7B μ•„ν‚¤ν…μ²
- GPT-NeoX κµ¬ν„

## π‰ κ²°λ΅ 

**κ¶μ¥ μ„¤μ •**: Medium λ¨λΈ (350M νλΌλ―Έν„°)
- 36GB λ©”λ¨λ¦¬μ—μ„ μ•μ •μ 
- 22μ‹κ°„ λ‚΄ ν•™μµ μ™„λ£ κ°€λ¥
- μ‹¤μ©μ μΈ μ¶”λ΅  μ†λ„
- ν•κµ­μ–΄ ν…μ¤νΈ μƒμ„±μ— μ¶©λ¶„ν• μ„±λ¥

**λ„μ „ κ³Όμ **: Large λ¨λΈ (800M νλΌλ―Έν„°)
- λ©”λ¨λ¦¬ μµλ€ ν™μ©
- λ” λ‚μ€ μ„±λ¥ κΈ°λ€
- 44μ‹κ°„ ν•™μµ μ‹κ°„
- Gradient checkpointing ν•„μ