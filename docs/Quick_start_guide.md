# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ

## ğŸš€ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### 1. Repository ìƒì„± ë° í´ë¡ 
```bash
# GitHubì—ì„œ ìƒˆ repository ìƒì„±
gh repo create korean-tiny-llm --public --clone

cd korean-tiny-llm
```

### 2. í”„ë¡œì íŠ¸ êµ¬ì¡° ìë™ ìƒì„±
```bash
# ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p docs src/{model,data,training,inference} scripts tests configs

# __init__.py íŒŒì¼ ìƒì„±
touch src/__init__.py
touch src/model/__init__.py
touch src/data/__init__.py
touch src/training/__init__.py
touch src/inference/__init__.py
```

### 3. requirements.txt ìƒì„±
```bash
cat > requirements.txt << 'EOF'
# Core
torch>=2.0.0
numpy>=1.24.0
tqdm>=4.65.0

# NLP
transformers>=4.30.0
tokenizers>=0.13.0
sentencepiece>=0.1.99

# Data
datasets>=2.12.0
pyarrow>=12.0.0

# Training
wandb>=0.15.0
pyyaml>=6.0

# Development
pytest>=7.3.0
black>=23.3.0
flake8>=6.0.0

# Optional: Optimization
# onnx>=1.14.0
# onnxruntime>=1.15.0
EOF
```

### 4. ê°€ìƒí™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
python3 -m venv venv
source venv/bin/activate
pip install --upgrade pip
pip install -r requirements.txt
```

### 5. ê¸°ë³¸ ì„¤ì • íŒŒì¼ ìƒì„±
```bash
# .gitignore
cat > .gitignore << 'EOF'
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
venv/
env/
ENV/

# Data
data/raw/
data/processed/
*.bin
*.pkl

# Models
checkpoints/
models/
*.pth
*.pt

# Logs
logs/
wandb/
*.log

# IDE
.vscode/
.idea/
*.swp

# OS
.DS_Store
EOF
```

### 6. README.md ìƒì„±
```bash
cat > README.md << 'EOF'
# Korean Tiny LLM

MacBook Airì—ì„œ í•™ìŠµ ê°€ëŠ¥í•œ ì†Œí˜• í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„± ëª¨ë¸

## Features
- 50M-150M íŒŒë¼ë¯¸í„° ê²½ëŸ‰ ëª¨ë¸
- ìµœì‹  Transformer ì•„í‚¤í…ì²˜ (RoPE, SwiGLU, GQA, RMSNorm)
- MacBook ìµœì í™” (M1/M2/M3)
- í•œêµ­ì–´ íŠ¹í™” í† í¬ë‚˜ì´ì €

## Quick Start
```bash
# ì„¤ì¹˜
pip install -r requirements.txt

# ë°ì´í„° ì¤€ë¹„
python scripts/prepare_data.py

# í•™ìŠµ
python scripts/train.py --config configs/training.yaml

# ì¶”ë¡ 
python scripts/generate.py --prompt "ì•ˆë…•í•˜ì„¸ìš”"
```

## Project Structure
See `docs/roadmap.md` for detailed information.

## License
MIT
EOF
```

### 7. ì²« ì»¤ë°‹
```bash
git add .
git commit -m "Initial project setup with structure and dependencies"
git push -u origin main
```

---

## ğŸ“‹ Phaseë³„ ìƒì„¸ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Phase 1: í”„ë¡œì íŠ¸ ì…‹ì—… âœ…

#### Day 1
- [ ] GitHub repository ìƒì„±
- [ ] ë¡œì»¬ í™˜ê²½ í´ë¡ 
- [ ] ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
- [ ] requirements.txt ì‘ì„±
- [ ] ê°€ìƒí™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜
- [ ] .gitignore ì„¤ì •
- [ ] README.md ì‘ì„±
- [ ] ì²« ì»¤ë°‹ ë° í‘¸ì‹œ

#### Day 2
- [ ] `docs/roadmap.md` ì‘ì„±
- [ ] `docs/architecture.md` ì´ˆì•ˆ ì‘ì„±
- [ ] ëª¨ë¸ config íŒŒì¼ ì‘ì„± (`configs/model_small.yaml`)
- [ ] í•™ìŠµ config íŒŒì¼ ì‘ì„± (`configs/training.yaml`)

---

### Phase 2: ì•„í‚¤í…ì²˜ êµ¬í˜„ ğŸ—ï¸

#### Week 1, Day 3-4: Tokenizer
```bash
# ì‘ì—… ëª©ë¡
- [ ] src/data/tokenizer.py êµ¬í˜„
  - [ ] SentencePiece ë˜í¼ í´ë˜ìŠ¤
  - [ ] í•œêµ­ì–´ normalizer
  - [ ] Vocab ê´€ë¦¬
- [ ] scripts/train_tokenizer.py êµ¬í˜„
- [ ] ìƒ˜í”Œ ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸
- [ ] tests/test_tokenizer.py ì‘ì„±
```

**ì½”ë“œ ìŠ¤ì¼ˆë ˆí†¤**:
```python
# src/data/tokenizer.py
from sentencepiece import SentencePieceProcessor

class KoreanTokenizer:
    def __init__(self, model_path):
        self.sp = SentencePieceProcessor()
        self.sp.load(model_path)
    
    def encode(self, text):
        # TODO: êµ¬í˜„
        pass
    
    def decode(self, ids):
        # TODO: êµ¬í˜„
        pass
```

#### Week 1, Day 5-7: Core Model Components

**Day 5: RoPE & RMSNorm**
```bash
- [ ] src/model/layers.py êµ¬í˜„
  - [ ] RotaryPositionEmbedding í´ë˜ìŠ¤
  - [ ] RMSNorm í´ë˜ìŠ¤
- [ ] ë‹¨ìœ„ í…ŒìŠ¤íŠ¸
```

**Day 6: Attention Mechanism**
```bash
- [ ] src/model/attention.py êµ¬í˜„
  - [ ] GroupedQueryAttention í´ë˜ìŠ¤
  - [ ] KV cache ì¤€ë¹„
- [ ] Shape í…ŒìŠ¤íŠ¸
```

**Day 7: Transformer Block**
```bash
- [ ] src/model/transformer.py êµ¬í˜„
  - [ ] TransformerBlock
  - [ ] SwiGLU FFN
  - [ ] ì „ì²´ ëª¨ë¸ í†µí•©
- [ ] Forward pass í…ŒìŠ¤íŠ¸
```

#### Week 2, Day 8-9: í†µí•© ë° í…ŒìŠ¤íŠ¸
```bash
- [ ] ëª¨ë¸ ì „ì²´ í†µí•©
- [ ] ë”ë¯¸ ë°ì´í„°ë¡œ forward/backward í…ŒìŠ¤íŠ¸
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í”„ë¡œíŒŒì¼ë§
- [ ] ë¬¸ì„œ ì—…ë°ì´íŠ¸ (architecture.md)
```

---

### Phase 3: ë°ì´í„° ì¤€ë¹„ ğŸ“Š

#### Week 2, Day 10-11: ë°ì´í„° ìˆ˜ì§‘
```bash
- [ ] ë°ì´í„° ì†ŒìŠ¤ ë¦¬ì„œì¹˜
  - [ ] AI Hub ê³„ì • ìƒì„± ë° ë°ì´í„° ë‹¤ìš´ë¡œë“œ
  - [ ] Korean Wikipedia ë¤í”„ ë‹¤ìš´ë¡œë“œ
  - [ ] ë¼ì´ì„ ìŠ¤ í™•ì¸
- [ ] data/raw/ ë””ë ‰í† ë¦¬ì— ì €ì¥
- [ ] ë°ì´í„° í’ˆì§ˆ ê°„ë‹¨íˆ í™•ì¸
```

**ì¶”ì²œ ë‹¤ìš´ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸**:
```bash
# scripts/download_data.sh
#!/bin/bash

# Korean Wikipedia
wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2
bzip2 -d kowiki-latest-pages-articles.xml.bz2

# ë‚˜ë¨¸ì§€ ì†ŒìŠ¤ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë˜ëŠ” API í™œìš©
```

#### Week 2, Day 12: ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸
```bash
- [ ] src/data/preprocessing.py êµ¬í˜„
  - [ ] HTML/XML íŒŒì‹±
  - [ ] í…ìŠ¤íŠ¸ ì •ì œ
  - [ ] ë¬¸ì¥ ë¶„ë¦¬
  - [ ] ì¤‘ë³µ ì œê±°
- [ ] scripts/prepare_data.py í†µí•©
```

#### Week 2, Day 13: Tokenizer í•™ìŠµ
```bash
- [ ] ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¡œ tokenizer í•™ìŠµ
- [ ] vocab size ì‹¤í—˜ (16k, 32k, 64k)
- [ ] í•œêµ­ì–´ í† í° ë¶„ì„
- [ ] ìµœì¢… tokenizer ì €ì¥
```

#### Week 2, Day 14: Dataset í´ë˜ìŠ¤
```bash
- [ ] src/data/dataset.py êµ¬í˜„
  - [ ] PyTorch Dataset ìƒì†
  - [ ] íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©
  - [ ] Collate function
- [ ] Train/Val split
- [ ] DataLoader í…ŒìŠ¤íŠ¸
```

---

### Phase 4: í•™ìŠµ ğŸ‹ï¸

#### Week 3, Day 15-16: í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
```bash
- [ ] src/training/trainer.py êµ¬í˜„
  - [ ] Training loop
  - [ ] Validation loop
  - [ ] Checkpoint ì €ì¥/ë¡œë“œ
  - [ ] Logging
- [ ] src/training/optimizer.py
  - [ ] AdamW with warmup
  - [ ] Learning rate scheduler
- [ ] scripts/train.py ì™„ì„±
```

#### Week 3, Day 17: Sanity Check
```bash
- [ ] ì‘ì€ ë°ì´í„°ì…‹(1MB)ìœ¼ë¡œ overfitting í…ŒìŠ¤íŠ¸
  - [ ] Lossê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ”ì§€ í™•ì¸
  - [ ] ìƒì„± í’ˆì§ˆ í™•ì¸
- [ ] ë²„ê·¸ ìˆ˜ì •
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì´ˆê¸° íŠœë‹
```

#### Week 3-4, Day 18-28: ë³¸ê²© í•™ìŠµ
```bash
# ë§¤ì¼
- [ ] í•™ìŠµ ì§„í–‰ ëª¨ë‹ˆí„°ë§
- [ ] Loss íŠ¸ë Œë“œ í™•ì¸
- [ ] ìƒì„± ìƒ˜í”Œ í‰ê°€
- [ ] ì´ìƒ í˜„ìƒ ëŒ€ì‘

# ì£¼ê°„
- [ ] Checkpoint í‰ê°€
- [ ] Validation perplexity í™•ì¸
- [ ] í•˜ì´í¼íŒŒë¼ë¯¸í„° ì¡°ì •
```

**í•™ìŠµ ëª¨ë‹ˆí„°ë§ ì²´í¬ë¦¬ìŠ¤íŠ¸**:
```
ì¼ì¼ ì²´í¬:
â–¡ Training loss ê°ì†Œ ì¤‘?
â–¡ Gradient norm ì•ˆì •ì ?
â–¡ GPU/Memory ì‚¬ìš©ë¥ ?
â–¡ ìƒì„± ìƒ˜í”Œ í’ˆì§ˆ ê°œì„ ?

ì£¼ê°„ ì²´í¬:
â–¡ Validation loss plateau?
â–¡ Learning rate ì¡°ì • í•„ìš”?
â–¡ ë°ì´í„° ì¶”ê°€ í•„ìš”?
â–¡ Early stopping ê³ ë ¤?
```

---

### Phase 5: ì¶”ë¡  ìµœì í™” ğŸš€

#### Week 5, Day 29-30: ê¸°ë³¸ ì¶”ë¡ 
```bash
- [ ] src/inference/generator.py êµ¬í˜„
  - [ ] Greedy decoding
  - [ ] Top-k/Top-p sampling
  - [ ] Temperature scaling
- [ ] scripts/generate.py CLI ë„êµ¬
- [ ] ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
```

#### Week 5, Day 31: ìµœì í™”
```bash
- [ ] KV cache êµ¬í˜„ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
- [ ] Batch inference ì§€ì›
- [ ] ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
```

#### Week 5, Day 32: ì¶”ê°€ ê¸°ëŠ¥
```bash
- [ ] Interactive chat mode
- [ ] API ì„œë²„ (FastAPI, ì„ íƒ)
- [ ] Web demo (Gradio, ì„ íƒ)
- [ ] ë¬¸ì„œí™” ì™„ë£Œ
```

---

## ğŸ¯ ê° Phase ì™„ë£Œ ê¸°ì¤€

### Phase 1 ì™„ë£Œ âœ…
- ëª¨ë“  íŒŒì¼ êµ¬ì¡° ì¡´ì¬
- requirements ì„¤ì¹˜ ì™„ë£Œ
- Git repository ì •ìƒ ì‘ë™

### Phase 2 ì™„ë£Œ âœ…
- ëª¨ë¸ forward pass ì‘ë™
- ëª¨ë“  ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼
- ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ 1 step ì„±ê³µ

### Phase 3 ì™„ë£Œ âœ…
- ìµœì†Œ 1GB ì´ìƒì˜ ì •ì œëœ ë°ì´í„°
- Tokenizer í•™ìŠµ ì™„ë£Œ
- DataLoaderì—ì„œ ë°°ì¹˜ ì •ìƒ ì¶œë ¥

### Phase 4 ì™„ë£Œ âœ…
- ìµœì†Œ 50k steps í•™ìŠµ ì™„ë£Œ
- Validation loss < 3.0
- ìƒì„± í…ìŠ¤íŠ¸ê°€ ë¬¸ë²•ì ìœ¼ë¡œ ìœ ì˜ë¯¸

### Phase 5 ì™„ë£Œ âœ…
- ì¶”ë¡  ì†ë„ > 10 tokens/sec
- Interactive mode ì‘ë™
- ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ†˜ ë¬¸ì œ í•´ê²° Quick Reference

### ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)
```yaml
# configs/training.yaml ìˆ˜ì •
batch_size: 2  # 4 â†’ 2
gradient_accumulation_steps: 16  # 8 â†’ 16
max_seq_length: 256  # 512 â†’ 256
gradient_checkpointing: true
```

### í•™ìŠµ ì†ë„ ëŠë¦¼
```python
# DataLoader ìµœì í™”
DataLoader(
    dataset,
    num_workers=2,  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ
    pin_memory=True,
    persistent_workers=True
)
```

### Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ
```yaml
# Learning rate ê°ì†Œ
learning_rate: 1e-4  # 3e-4 â†’ 1e-4

# Warmup ì¦ê°€
warmup_steps: 5000  # 2000 â†’ 5000

# Gradient clipping
max_grad_norm: 0.5  # 1.0 â†’ 0.5
```

### ìƒì„± í’ˆì§ˆ ë‚®ìŒ
1. ë” ë§ì€ ë°ì´í„° ì¶”ê°€
2. ë” ê¸´ ì‹œê°„ í•™ìŠµ
3. Validation lossê°€ ì¶©ë¶„íˆ ë‚®ì€ì§€ í™•ì¸
4. Temperature ì¡°ì • (0.7 ~ 1.0 ì‹¤í—˜)

---

## ğŸ“Š Progress Tracking

### ì§„í–‰ ìƒí™© ì¶”ì  í…œí”Œë¦¿
```markdown
## ì£¼ì°¨ë³„ ì§„í–‰ ìƒí™©

### Week 1 (MM/DD - MM/DD)
- [x] Repository ì…‹ì—…
- [x] ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„
- [ ] Tokenizer êµ¬í˜„
- [ ] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘

**ì´ìŠˆ**:
- 

**ë‹¤ìŒ ì£¼ ê³„íš**:
- 
```

---

## ğŸ“ í•™ìŠµ ë¦¬ì†ŒìŠ¤

### í•„ë… ìë£Œ
1. "Attention Is All You Need" ë…¼ë¬¸
2. Andrej Karpathy's nanoGPT
3. Hugging Face Transformers ì½”ë“œ

### ì¶”ì²œ ê°•ì˜
- Stanford CS224N (NLP)
- Fast.ai Deep Learning

### ì»¤ë®¤ë‹ˆí‹°
- Hugging Face Discord
- Reddit r/MachineLearning
- Papers With Code

---

**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸ë¶€í„° ì‹œì‘í•˜ì„¸ìš”!