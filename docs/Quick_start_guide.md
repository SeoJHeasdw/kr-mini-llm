# ë¹ ë¥¸ ì‹œì‘ ê°€ì´ë“œ (M4 Max ìµœì í™”)

> **í•˜ë“œì›¨ì–´**: MacBook Pro 16" M4 Max (36GB RAM, 32-core GPU)
> **ëª©í‘œ**: 350M-1B íŒŒë¼ë¯¸í„° í•œêµ­ì–´ LLM êµ¬ì¶•

## ğŸš€ 5ë¶„ ì•ˆì— ì‹œì‘í•˜ê¸°

### ì „ì œ ì¡°ê±´
- âœ… MacBook Pro M4 Max (36GB RAM ì´ìƒ)
- âœ… Python 3.13+
- âœ… 20GB+ ë””ìŠ¤í¬ ì—¬ìœ  ê³µê°„

### 1. í”„ë¡œì íŠ¸ í´ë¡  (ì´ë¯¸ ì™„ë£Œëœ ê²½ìš° ìŠ¤í‚µ)
```bash
cd kr-mini-llm
```

### 2. í”„ë¡œì íŠ¸ êµ¬ì¡° í™•ì¸
```bash
# ì´ë¯¸ ìƒì„±ëœ êµ¬ì¡° í™•ì¸
ls -la configs/  # ì„¤ì • íŒŒì¼ë“¤
ls -la src/      # ì†ŒìŠ¤ ì½”ë“œ
ls -la docs/     # ë¬¸ì„œ
```

**ì£¼ìš” ì„¤ì • íŒŒì¼:**
- `configs/model_medium.yaml` - 468M íŒŒë¼ë¯¸í„° (ê¶Œì¥)
- `configs/model_large.yaml` - 1004M íŒŒë¼ë¯¸í„° (ë„ì „)
- `configs/training_m4max.yaml` - Medium ëª¨ë¸ í•™ìŠµ ì„¤ì •
- `configs/training_m4max_large.yaml` - Large ëª¨ë¸ í•™ìŠµ ì„¤ì •

### 3. requirements.txt í™•ì¸
```bash
cat requirements.txt
```

**ì´ë¯¸ í¬í•¨ëœ ì£¼ìš” íŒ¨í‚¤ì§€:**
- PyTorch 2.0+ (MPS ì§€ì›)
- Transformers, Tokenizers
- SentencePiece (í•œêµ­ì–´ í† í¬ë‚˜ì´ì €)
- Weights & Biases (í•™ìŠµ ëª¨ë‹ˆí„°ë§)

### 4. ê°€ìƒí™˜ê²½ ì„¤ì • ë° íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
# ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv

# í™œì„±í™”
source venv/bin/activate

# íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt

# PyTorch MPS ì§€ì› í™•ì¸
python3 -c "import torch; print(f'PyTorch: {torch.__version__}'); print(f'MPS available: {torch.backends.mps.is_available()}')"
```

**ì˜ˆìƒ ì¶œë ¥:**
```
PyTorch: 2.x.x
MPS available: True
```

### 5. ì„¤ì • í…ŒìŠ¤íŠ¸
```bash
# ì„¤ì • íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
python3 scripts/test_config.py
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!
- âœ… Small ëª¨ë¸: ~134M íŒŒë¼ë¯¸í„°
- âœ… Medium ëª¨ë¸: ~468M íŒŒë¼ë¯¸í„°
- âœ… Large ëª¨ë¸: ~1004M íŒŒë¼ë¯¸í„°
```

### 6. í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ í…ŒìŠ¤íŠ¸
```bash
# Medium ëª¨ë¸ ì„¤ì • í™•ì¸
python3 scripts/train.py \
  --config configs/training_m4max.yaml \
  --model_config configs/model_medium.yaml
```

**ì˜ˆìƒ ì¶œë ¥:**
```
ğŸš€ M4 Max Metal Performance Shaders (MPS) ì‚¬ìš©
   MPS ë©”ëª¨ë¦¬ ì œí•œ: 80%

ğŸ¤– ëª¨ë¸: 1024 hidden, 24 layers, 16 heads
ğŸ‹ï¸  í•™ìŠµ: batch=16, effective_batch=64
ğŸ’» ë””ë°”ì´ìŠ¤: mps (MPS ìµœì í™” í™œì„±í™”)
```

---

## ğŸ“‹ Phaseë³„ ì²´í¬ë¦¬ìŠ¤íŠ¸ (M4 Max ê¸°ì¤€)

### Phase 1: í”„ë¡œì íŠ¸ ì…‹ì—… âœ… (ì™„ë£Œ)

- [x] í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
- [x] M4 Max ìµœì í™” ì„¤ì • íŒŒì¼ ìƒì„±
- [x] ë¬¸ì„œ ì‘ì„± (ë¡œë“œë§µ, ê°€ì´ë“œ)
- [x] í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ì‘ì„±
- [x] MPS ì§€ì› ì¶”ê°€

**ìƒì„±ëœ íŒŒì¼:**
- `configs/model_medium.yaml`, `model_large.yaml`
- `configs/training_m4max.yaml`, `training_m4max_large.yaml`
- `src/model/config.py` (YAML ë¡œë“œ, í”„ë¦¬ì…‹)
- `scripts/train.py` (MPS ì§€ì›)
- `scripts/test_config.py` (í…ŒìŠ¤íŠ¸)

---

### Phase 2: ì•„í‚¤í…ì²˜ êµ¬í˜„ ğŸ—ï¸ (3-5ì¼)

**ëª©í‘œ**: Medium ëª¨ë¸ (468M) ì•„í‚¤í…ì²˜ ì™„ì„±

#### ìš°ì„ ìˆœìœ„ 1: í•µì‹¬ ë ˆì´ì–´ (1-2ì¼)
```bash
# src/model/layers.py êµ¬í˜„
- [ ] RMSNorm - Layer normalization ëŒ€ì²´
- [ ] RotaryPositionEmbedding (RoPE) - ìœ„ì¹˜ ì¸ì½”ë”©
- [ ] SwiGLU - Feed-forward í™œì„±í™” í•¨ìˆ˜

# í…ŒìŠ¤íŠ¸
python3 -m pytest tests/test_layers.py -v
```

**M4 Max ìµœì í™” í¬ì¸íŠ¸:**
- MPS ë°±ì—”ë“œ í˜¸í™˜ì„± í™•ì¸
- FP16 mixed precision ì§€ì›
- torch.compile ì ìš© ê°€ëŠ¥ì„±

#### ìš°ì„ ìˆœìœ„ 2: Attention (1-2ì¼)
```bash
# src/model/attention.py êµ¬í˜„
- [ ] GroupedQueryAttention (GQA)
  - num_heads=16, num_kv_heads=4 (Medium)
  - num_heads=24, num_kv_heads=6 (Large)
- [ ] KV cache êµ¬í˜„ (ì¶”ë¡  ìµœì í™”)
- [ ] Causal masking

# ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
python3 scripts/profile_attention.py
```

**ì˜ˆìƒ ë©”ëª¨ë¦¬ (Medium):**
- Attention weights: ~2GB
- KV cache: ~1GB (ì¶”ë¡  ì‹œ)

#### ìš°ì„ ìˆœìœ„ 3: Transformer í†µí•© (1ì¼)
```bash
# src/model/transformer.py êµ¬í˜„
- [ ] TransformerBlock (Attention + FFN)
- [ ] TransformerLM (ì „ì²´ ëª¨ë¸)
- [ ] Forward/Backward pass ê²€ì¦

# ë”ë¯¸ ë°ì´í„° í…ŒìŠ¤íŠ¸
python3 scripts/test_model_forward.py
```

**ê²€ì¦ í•­ëª©:**
- [ ] 468M íŒŒë¼ë¯¸í„° í™•ì¸
- [ ] MPSì—ì„œ forward pass ì„±ê³µ
- [ ] Gradient ê³„ì‚° ì •ìƒ
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~10GB ì´ë‚´

---

### Phase 3: ë°ì´í„° ì¤€ë¹„ ğŸ“Š (2-3ì¼)

**ëª©í‘œ**: 20-50GB í•œêµ­ì–´ í…ìŠ¤íŠ¸ ë°ì´í„° í™•ë³´

#### ë°ì´í„° ì†ŒìŠ¤ (ìš°ì„ ìˆœìœ„ ìˆœ)

**1. AI Hub (ìµœìš°ì„ )**
```bash
# https://aihub.or.kr
- [ ] ì¼ìƒëŒ€í™” ë°ì´í„° (~5GB)
- [ ] ë¬¸ì„œìš”ì•½ ë°ì´í„° (~3GB)
- [ ] ë‰´ìŠ¤ ê¸°ì‚¬ ë°ì´í„° (~10GB)
```

**2. Korean Wikipedia**
```bash
# ìœ„í‚¤ë¯¸ë””ì–´ ë¤í”„
wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2
bzip2 -d kowiki-latest-pages-articles.xml.bz2

# ì˜ˆìƒ í¬ê¸°: ~2GB (ì••ì¶• í•´ì œ í›„ ~10GB)
```

**3. ë‚˜ë¬´ìœ„í‚¤ (ì„ íƒ)**
```bash
# í¬ë¡¤ë§ í•„ìš” (robots.txt í™•ì¸)
# ì˜ˆìƒ í¬ê¸°: ~20GB
```

**4. ëª¨ë‘ì˜ ë§ë­‰ì¹˜ (êµ­ë¦½êµ­ì–´ì›)**
```bash
# https://corpus.korean.go.kr
- [ ] ì‹ ë¬¸ ë§ë­‰ì¹˜
- [ ] ë¬¸ì–´ ë§ë­‰ì¹˜
```

#### ë°ì´í„° ì „ì²˜ë¦¬ (1ì¼)
```bash
# scripts/prepare_data.py ì‹¤í–‰
python3 scripts/prepare_data.py \
  --input data/raw/*.txt \
  --output data/processed/ \
  --min_length 10 \
  --max_length 2048

# ì˜ˆìƒ ì²˜ë¦¬ ì‹œê°„: 2-4ì‹œê°„ (M4 Max)
```

**ì „ì²˜ë¦¬ ë‹¨ê³„:**
1. HTML/XML íƒœê·¸ ì œê±°
2. íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
3. ì¤‘ë³µ ë¬¸ì¥ ì œê±°
4. Train/Val/Test ë¶„í•  (98/1/1)

#### Tokenizer í•™ìŠµ (1ì¼)
```bash
# 32k vocab SentencePiece í•™ìŠµ
python3 scripts/train_tokenizer.py \
  --input data/processed/train.txt \
  --vocab_size 32000 \
  --model_type bpe \
  --output models/tokenizer

# ì˜ˆìƒ í•™ìŠµ ì‹œê°„: 1-2ì‹œê°„
```

**ê²€ì¦:**
```python
from src.data.tokenizer import KoreanTokenizer

tokenizer = KoreanTokenizer("models/tokenizer.model")
text = "ì•ˆë…•í•˜ì„¸ìš”, M4 Maxì—ì„œ í•™ìŠµí•˜ëŠ” í•œêµ­ì–´ LLMì…ë‹ˆë‹¤."
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")
print(f"Decoded: {tokenizer.decode(tokens)}")
```
```bash
# scripts/download_data.sh
#!/bin/bash

# Korean Wikipedia
wget https://dumps.wikimedia.org/kowiki/latest/kowiki-latest-pages-articles.xml.bz2
bzip2 -d kowiki-latest-pages-articles.xml.bz2

# ë‚˜ë¨¸ì§€ ì†ŒìŠ¤ëŠ” ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ ë˜ëŠ” API í™œìš©
```

---

### Phase 4: í•™ìŠµ ğŸ‹ï¸ (2-3ì£¼)

**ëª©í‘œ**: Medium ëª¨ë¸ 200k steps í•™ìŠµ ì™„ë£Œ (~22ì‹œê°„)

#### í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶• (1-2ì¼)
```bash
# src/training/trainer.py êµ¬í˜„
- [ ] Training loop (MPS ìµœì í™”)
- [ ] Validation loop
- [ ] Checkpoint ì €ì¥/ë¡œë“œ
- [ ] W&B ë¡œê¹… (ì„ íƒ)

# src/training/optimizer.py
- [ ] AdamW (betas=[0.9, 0.95])
- [ ] Cosine learning rate scheduler
- [ ] Gradient clipping (max_norm=1.0)
```

#### Sanity Check (1ì¼)
```bash
# ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ overfitting í…ŒìŠ¤íŠ¸
python3 scripts/train.py \
  --config configs/training_m4max.yaml \
  --model_config configs/model_medium.yaml \
  --max_steps 1000 \
  --data_size 1MB

# í™•ì¸ ì‚¬í•­:
- [ ] Lossê°€ 0ì— ê°€ê¹Œì›Œì§€ëŠ”ê°€?
- [ ] ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ~10GB ì´ë‚´?
- [ ] í•™ìŠµ ì†ë„ 2-3 steps/sec?
```

#### ë³¸ê²© í•™ìŠµ (Medium ëª¨ë¸)
```bash
# í•™ìŠµ ì‹œì‘
python3 scripts/train.py \
  --config configs/training_m4max.yaml \
  --model_config configs/model_medium.yaml

# ì˜ˆìƒ ì‹œê°„: ~22ì‹œê°„ (200k steps)
# ì˜ˆìƒ ë©”ëª¨ë¦¬: ~10-15GB
# í•™ìŠµ ì†ë„: 2-3 steps/sec
```

**M4 Max ìµœì í™” ì„¤ì • (ì´ë¯¸ ì ìš©ë¨):**
- âœ… Batch size: 16
- âœ… Gradient accumulation: 4 (effective batch=64)
- âœ… Sequence length: 2048
- âœ… Mixed precision: FP16
- âœ… Gradient checkpointing: OFF (ì†ë„ ìš°ì„ )
- âœ… torch.compile: ON
- âœ… MPS ë©”ëª¨ë¦¬ ì œí•œ: 80%

#### í•™ìŠµ ëª¨ë‹ˆí„°ë§
```bash
# W&B ëŒ€ì‹œë³´ë“œ (ì„ íƒ)
# configs/training_m4max.yamlì—ì„œ wandb.enabled: true

# ë¡œì»¬ ëª¨ë‹ˆí„°ë§
tail -f checkpoints/medium/train.log

# ì²´í¬í¬ì¸íŠ¸ í™•ì¸
ls -lh checkpoints/medium/checkpoint-*
```

**ì¼ì¼ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Training loss ê°ì†Œ ì¶”ì„¸?
- [ ] Validation perplexity < 20?
- [ ] MPS ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥  < 80%?
- [ ] ìƒì„± ìƒ˜í”Œ í’ˆì§ˆ ê°œì„ ?
- [ ] Gradient norm ì•ˆì •ì ? (< 5.0)

**ì£¼ê°„ ì²´í¬ë¦¬ìŠ¤íŠ¸:**
- [ ] Checkpoint í‰ê°€ (5k stepsë§ˆë‹¤)
- [ ] Learning rate ì¡°ì • í•„ìš”?
- [ ] ë°ì´í„° ì¶”ê°€ í•„ìš”?
- [ ] Early stopping ê³ ë ¤?

#### í•™ìŠµ ì¬ê°œ (ì¤‘ë‹¨ ì‹œ)
```bash
python3 scripts/train.py \
  --config configs/training_m4max.yaml \
  --model_config configs/model_medium.yaml \
  --resume_from checkpoints/medium/checkpoint-50000
```

---

### Phase 5: ì¶”ë¡  ìµœì í™” ğŸš€ (2-3ì¼)

**ëª©í‘œ**: ì¶”ë¡  ì†ë„ > 50 tokens/sec (Medium)

#### ê¸°ë³¸ ì¶”ë¡  êµ¬í˜„ (1ì¼)
```bash
# src/inference/generator.py êµ¬í˜„
- [ ] Greedy decoding
- [ ] Top-k/Top-p sampling
- [ ] Temperature scaling
- [ ] KV cache (ë©”ëª¨ë¦¬ íš¨ìœ¨)

# í…ŒìŠ¤íŠ¸
python3 scripts/generate.py \
  --checkpoint checkpoints/medium/final \
  --prompt "ì•ˆë…•í•˜ì„¸ìš”, M4 Maxì—ì„œ" \
  --max_length 100
```

#### ì¶”ë¡  ìµœì í™” (1ì¼)
```bash
# ìµœì í™” ê¸°ë²•
- [ ] KV cache êµ¬í˜„ (í•„ìˆ˜)
- [ ] torch.compile ì ìš©
- [ ] Batch inference ì§€ì›
- [ ] MPS ìµœì í™”

# ë²¤ì¹˜ë§ˆí¬
python3 scripts/benchmark_inference.py
```

**ì˜ˆìƒ ì„±ëŠ¥ (Medium, M4 Max):**
- Tokens/sec: 50-80
- Latency (ì²« í† í°): < 100ms
- Memory: ~5GB

#### Interactive Demo (ì„ íƒ)
```bash
# Gradio ì›¹ ì¸í„°í˜ì´ìŠ¤
pip install gradio
python3 scripts/demo.py --port 7860

# ë˜ëŠ” CLI
python3 scripts/chat.py
```
  - [ ] Greedy decoding
  - [ ] Top-k/Top-p sampling
  - [ ] Temperature scaling
- [ ] scripts/generate.py CLI ë„êµ¬
- [ ] ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ í…ŒìŠ¤íŠ¸
```

#### Week 5, Day 31: ìµœì í™”
```bash
- [ ] KV cache êµ¬í˜„ (ë©”ëª¨ë¦¬ íš¨ìœ¨)
- [ ] Batch inference ì§€ì›
- [ ] ì¶”ë¡  ì†ë„ ë²¤ì¹˜ë§ˆí¬
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§
```

#### Week 5, Day 32: ì¶”ê°€ ê¸°ëŠ¥
```bash
- [ ] Interactive chat mode
- [ ] API ì„œë²„ (FastAPI, ì„ íƒ)
- [ ] Web demo (Gradio, ì„ íƒ)
- [ ] ë¬¸ì„œí™” ì™„ë£Œ
```

---

## ğŸ¯ ê° Phase ì™„ë£Œ ê¸°ì¤€

### Phase 1 ì™„ë£Œ âœ…
- ëª¨ë“  íŒŒì¼ êµ¬ì¡° ì¡´ì¬
- requirements ì„¤ì¹˜ ì™„ë£Œ
- Git repository ì •ìƒ ì‘ë™

### Phase 2 ì™„ë£Œ âœ…
- ëª¨ë¸ forward pass ì‘ë™
- ëª¨ë“  ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼
- ë”ë¯¸ ë°ì´í„°ë¡œ í•™ìŠµ 1 step ì„±ê³µ

### Phase 3 ì™„ë£Œ âœ…
- ìµœì†Œ 1GB ì´ìƒì˜ ì •ì œëœ ë°ì´í„°
- Tokenizer í•™ìŠµ ì™„ë£Œ
- DataLoaderì—ì„œ ë°°ì¹˜ ì •ìƒ ì¶œë ¥

### Phase 4 ì™„ë£Œ âœ…
- ìµœì†Œ 50k steps í•™ìŠµ ì™„ë£Œ
- Validation loss < 3.0
- ìƒì„± í…ìŠ¤íŠ¸ê°€ ë¬¸ë²•ì ìœ¼ë¡œ ìœ ì˜ë¯¸

### Phase 5 ì™„ë£Œ âœ…
- ì¶”ë¡  ì†ë„ > 10 tokens/sec
- Interactive mode ì‘ë™
- ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ†˜ ë¬¸ì œ í•´ê²° Quick Reference

### ë©”ëª¨ë¦¬ ë¶€ì¡± (OOM)
```yaml
# configs/training.yaml ìˆ˜ì •
batch_size: 2  # 4 â†’ 2
gradient_accumulation_steps: 16  # 8 â†’ 16
max_seq_length: 256  # 512 â†’ 256
gradient_checkpointing: true
```

### í•™ìŠµ ì†ë„ ëŠë¦¼
```python
# DataLoader ìµœì í™”
DataLoader(
    dataset,
    num_workers=2,  # CPU ì½”ì–´ ìˆ˜ì— ë§ê²Œ
    pin_memory=True,
    persistent_workers=True
)
```

### Lossê°€ ê°ì†Œí•˜ì§€ ì•ŠìŒ
```yaml
# Learning rate ê°ì†Œ
learning_rate: 1e-4  # 3e-4 â†’ 1e-4

# Warmup ì¦ê°€
warmup_steps: 5000  # 2000 â†’ 5000

# Gradient clipping
max_grad_norm: 0.5  # 1.0 â†’ 0.5
```

### ìƒì„± í’ˆì§ˆ ë‚®ìŒ
1. ë” ë§ì€ ë°ì´í„° ì¶”ê°€
2. ë” ê¸´ ì‹œê°„ í•™ìŠµ
3. Validation lossê°€ ì¶©ë¶„íˆ ë‚®ì€ì§€ í™•ì¸
4. Temperature ì¡°ì • (0.7 ~ 1.0 ì‹¤í—˜)

---

## ğŸ“Š Progress Tracking

### ì§„í–‰ ìƒí™© ì¶”ì  í…œí”Œë¦¿
```markdown
## ì£¼ì°¨ë³„ ì§„í–‰ ìƒí™©

### Week 1 (MM/DD - MM/DD)
- [x] Repository ì…‹ì—…
- [x] ëª¨ë¸ ì•„í‚¤í…ì²˜ ì„¤ê³„
- [ ] Tokenizer êµ¬í˜„
- [ ] ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘

**ì´ìŠˆ**:
- 

**ë‹¤ìŒ ì£¼ ê³„íš**:
- 
```

---

## ğŸ“ í•™ìŠµ ë¦¬ì†ŒìŠ¤

### í•„ë… ìë£Œ
1. "Attention Is All You Need" ë…¼ë¬¸
2. Andrej Karpathy's nanoGPT
3. Hugging Face Transformers ì½”ë“œ

### ì¶”ì²œ ê°•ì˜
- Stanford CS224N (NLP)
- Fast.ai Deep Learning

### ì»¤ë®¤ë‹ˆí‹°
- Hugging Face Discord
- Reddit r/MachineLearning
- Papers With Code

---

**ë‹¤ìŒ ë‹¨ê³„**: Phase 1 ì²´í¬ë¦¬ìŠ¤íŠ¸ë¶€í„° ì‹œì‘í•˜ì„¸ìš”!