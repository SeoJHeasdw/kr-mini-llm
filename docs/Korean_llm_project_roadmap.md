# í•œêµ­ì–´ LLM í”„ë¡œì íŠ¸ ë¡œë“œë§µ

## í”„ë¡œì íŠ¸ ê°œìš”
MacBook Airì—ì„œ êµ¬ë™ ê°€ëŠ¥í•œ ì†Œí˜• í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„± LLM êµ¬ì¶•

**ëª©í‘œ ìŠ¤í™**
- ëª¨ë¸ í¬ê¸°: 50M-150M íŒŒë¼ë¯¸í„°
- íƒ€ê²Ÿ ë””ë°”ì´ìŠ¤: MacBook Air (M1/M2/M3)
- ì–¸ì–´: í•œêµ­ì–´ í…ìŠ¤íŠ¸ ìƒì„±
- ì•„í‚¤í…ì²˜: ìµœì‹  Transformer ë³€í˜• (RoPE, SwiGLU, RMSNorm, GQA)

---

## ğŸ“‹ Phase 1: í”„ë¡œì íŠ¸ ì…‹ì—… (1-2ì¼)

### 1.1 Repository êµ¬ì¡° ìƒì„±
```
korean-tiny-llm/
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.md
â”‚   â”œâ”€â”€ training-guide.md
â”‚   â””â”€â”€ inference-guide.md
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ transformer.py
â”‚   â”‚   â”œâ”€â”€ attention.py
â”‚   â”‚   â””â”€â”€ config.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ tokenizer.py
â”‚   â”‚   â””â”€â”€ dataset.py
â”‚   â”œâ”€â”€ training/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ trainer.py
â”‚   â”‚   â””â”€â”€ optimizer.py
â”‚   â””â”€â”€ inference/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ generator.py
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ prepare_data.py
â”‚   â”œâ”€â”€ train.py
â”‚   â””â”€â”€ generate.py
â”œâ”€â”€ tests/
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ model_small.yaml
â”‚   â””â”€â”€ training.yaml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore
```

### 1.2 í™˜ê²½ ì„¤ì •
```bash
# Python ê°€ìƒí™˜ê²½ ìƒì„±
python -m venv venv
source venv/bin/activate

# í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install torch torchvision torchaudio
pip install transformers
pip install tokenizers
pip install datasets
pip install wandb  # ì„ íƒ: í•™ìŠµ ëª¨ë‹ˆí„°ë§
pip install tqdm numpy pyyaml
```

### 1.3 Git ì´ˆê¸°í™”
```bash
git init
git add .
git commit -m "Initial project structure"
```

---

## ğŸ“ Phase 2: ì•„í‚¤í…ì²˜ ì„¤ê³„ ë° êµ¬í˜„ (3-5ì¼)

### 2.1 ëª¨ë¸ ì„¤ê³„ ë¬¸ì„œ ì‘ì„±
- `docs/architecture.md` ì‘ì„±
- í•˜ì´í¼íŒŒë¼ë¯¸í„° ê²°ì •
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°

**ì¶”ì²œ ëª¨ë¸ ì„¤ì • (Small)**
```yaml
vocab_size: 32000
hidden_size: 768
num_layers: 12
num_heads: 12
num_kv_heads: 4  # GQA
intermediate_size: 2048  # SwiGLU
max_seq_length: 1024
rope_theta: 10000.0
```

### 2.2 í•µì‹¬ ì»´í¬ë„ŒíŠ¸ êµ¬í˜„

**ìš°ì„ ìˆœìœ„ 1: Tokenizer**
- SentencePiece ë˜ëŠ” BPE ê¸°ë°˜
- í•œêµ­ì–´ ìµœì í™” vocab 32k
- `src/data/tokenizer.py`

**ìš°ì„ ìˆœìœ„ 2: Model Architecture**
- RoPE Position Encoding
- Multi-Head Attention with GQA
- SwiGLU Feed-Forward
- RMSNorm
- `src/model/transformer.py`

**ìš°ì„ ìˆœìœ„ 3: Data Pipeline**
- í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬
- ë°ì´í„°ì…‹ ë¡œë”
- `src/data/dataset.py`

### 2.3 ìœ ë‹› í…ŒìŠ¤íŠ¸ ì‘ì„±
```python
# tests/test_model.py
def test_model_forward():
    pass

def test_attention_shape():
    pass

def test_rope_encoding():
    pass
```

---

## ğŸ“Š Phase 3: ë°ì´í„° ì¤€ë¹„ (2-3ì¼)

### 3.1 ë°ì´í„° ì†ŒìŠ¤ ì„ ì •

**ì¶”ì²œ í•œêµ­ì–´ ë°ì´í„°ì…‹**
1. **AI Hub ê³µê°œ ë°ì´í„°** (ìš°ì„ ìˆœìœ„ ë†’ìŒ)
   - ì¼ìƒëŒ€í™” ë°ì´í„°
   - ë¬¸ì„œìš”ì•½ ë°ì´í„°
   
2. **ë‚˜ë¬´ìœ„í‚¤ ë¤í”„** (ë°±ì—…ìš©)
   - í¬ë¡¤ë§ í›„ ì „ì²˜ë¦¬
   
3. **ëª¨ë‘ì˜ ë§ë­‰ì¹˜** (êµ­ë¦½êµ­ì–´ì›)
   - ì‹ ë¬¸, ë¬¸ì–´ì²´ ë°ì´í„°

4. **Korean Wikipedia**
   - ìœ„í‚¤ë¯¸ë””ì–´ ë¤í”„

**ëª©í‘œ ë°ì´í„° í¬ê¸°**: ìµœì†Œ 1GB, ì´ìƒì ìœ¼ë¡œ 5-10GB

### 3.2 ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸

```python
# scripts/prepare_data.pyì˜ ë‹¨ê³„

# 1. Raw ë°ì´í„° ë‹¤ìš´ë¡œë“œ
# 2. í…ìŠ¤íŠ¸ ì •ì œ
#    - HTML íƒœê·¸ ì œê±°
#    - íŠ¹ìˆ˜ë¬¸ì ì •ê·œí™”
#    - ì¤‘ë³µ ì œê±°
# 3. ë¬¸ì¥ ë¶„ë¦¬ ë° í•„í„°ë§
# 4. Train/Val/Test ë¶„í•  (98/1/1)
# 5. Tokenization
# 6. Binary í˜•ì‹ ì €ì¥ (íš¨ìœ¨ì„±)
```

### 3.3 Tokenizer í•™ìŠµ
```bash
python scripts/train_tokenizer.py \
  --input data/raw/*.txt \
  --vocab_size 32000 \
  --output models/tokenizer
```

---

## ğŸ‹ï¸ Phase 4: í•™ìŠµ (5-10ì¼)

### 4.1 í•™ìŠµ ì„¤ì •

**MacBook Air ìµœì í™”**
```yaml
# configs/training.yaml
batch_size: 4  # Gradient accumulationìœ¼ë¡œ ì¦ê°€
gradient_accumulation_steps: 8  # Effective batch = 32
max_seq_length: 512  # ì´ˆê¸°ì—ëŠ” ì§§ê²Œ
learning_rate: 3e-4
warmup_steps: 2000
max_steps: 100000
save_steps: 5000
eval_steps: 1000
fp16: true  # Mixed precision
gradient_checkpointing: true  # ë©”ëª¨ë¦¬ ì ˆì•½
```

### 4.2 í•™ìŠµ ì‹¤í–‰
```bash
# ë‹¨ì¼ GPU í•™ìŠµ
python scripts/train.py \
  --config configs/training.yaml \
  --model_config configs/model_small.yaml \
  --output_dir checkpoints/

# í•™ìŠµ ì¬ê°œ
python scripts/train.py \
  --resume_from checkpoints/checkpoint-5000/
```

### 4.3 ëª¨ë‹ˆí„°ë§
- Weights & Biases ëŒ€ì‹œë³´ë“œ
- Loss íŠ¸ë˜í‚¹
- Perplexity ì¸¡ì •
- ìƒ˜í”Œ ìƒì„± í™•ì¸

### 4.4 í•™ìŠµ íŒ
- **ì²« 24ì‹œê°„**: Overfitting í™•ì¸ìš© ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
- **ì¤‘ê°„ í‰ê°€**: 5k stepsë§ˆë‹¤ ìƒì„± í’ˆì§ˆ í™•ì¸
- **í•™ìŠµë¥  ì¡°ì •**: Loss plateau ì‹œ ê°ì†Œ
- **Early stopping**: Validation loss ìƒìŠ¹ ì‹œ ì¤‘ë‹¨

---

## ğŸš€ Phase 5: ì¶”ë¡  ë° ìµœì í™” (2-3ì¼)

### 5.1 ê¸°ë³¸ ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤
```python
# scripts/generate.py
from src.inference import Generator

generator = Generator.from_pretrained("checkpoints/final")

text = generator.generate(
    prompt="ì˜¤ëŠ˜ ë‚ ì”¨ê°€",
    max_length=100,
    temperature=0.8,
    top_p=0.95
)
print(text)
```

### 5.2 ì¶”ë¡  ìµœì í™”
- **KV Cache êµ¬í˜„**: ë°˜ë³µ ê³„ì‚° ì œê±°
- **Dynamic Batching**: íš¨ìœ¨ì  ì²˜ë¦¬
- **ì–‘ìí™” (ì„ íƒ)**: INT8 ì–‘ìí™”ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½
- **ONNX ë³€í™˜ (ì„ íƒ)**: ì¶”ë¡  ì†ë„ í–¥ìƒ

### 5.3 ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```python
# ì¸¡ì • í•­ëª©
- Tokens/sec (ìƒì„± ì†ë„)
- Latency (ì²« í† í°ê¹Œì§€ ì‹œê°„)
- Memory usage (Peak RAM)
- Model size on disk
```

---

## ğŸ“ˆ Phase 6: í‰ê°€ ë° ê°œì„  (ì§„í–‰í˜•)

### 6.1 ì •ì„± í‰ê°€
- ë‹¤ì–‘í•œ í”„ë¡¬í”„íŠ¸ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸
- ë¬¸ë²• ì •í™•ë„
- ë¬¸ë§¥ ì¼ê´€ì„±
- ì°½ì˜ì„±

### 6.2 ì •ëŸ‰ í‰ê°€ (ì„ íƒ)
- Perplexity on test set
- BLEU/ROUGE (íŠ¹ì • íƒœìŠ¤í¬)
- KoBERT ê¸°ë°˜ ìœ ì‚¬ë„

### 6.3 ê°œì„  ë°©í–¥
- ë” ë§ì€ ë°ì´í„°ë¡œ ì¬í•™ìŠµ
- í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- Instruction tuning (ì¶”ê°€ ë‹¨ê³„)
- RLHF (ê³ ê¸‰)

---

## ğŸ¯ ë§ˆì¼ìŠ¤í†¤ ì²´í¬ë¦¬ìŠ¤íŠ¸

### Week 1
- [ ] Repository êµ¬ì¡° ì™„ì„±
- [ ] ê¸°ë³¸ ëª¨ë¸ ì•„í‚¤í…ì²˜ êµ¬í˜„
- [ ] ìœ ë‹› í…ŒìŠ¤íŠ¸ í†µê³¼
- [ ] ë°ì´í„° ì†ŒìŠ¤ í™•ë³´

### Week 2
- [ ] Tokenizer í•™ìŠµ ì™„ë£Œ
- [ ] ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ
- [ ] í•™ìŠµ íŒŒì´í”„ë¼ì¸ êµ¬ì¶•
- [ ] ì‘ì€ ë°ì´í„°ì…‹ìœ¼ë¡œ Overfitting í™•ì¸

### Week 3-4
- [ ] ì „ì²´ ë°ì´í„°ì…‹ í•™ìŠµ ì‹œì‘
- [ ] ì¤‘ê°„ ì²´í¬í¬ì¸íŠ¸ í‰ê°€
- [ ] í•™ìŠµ ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •

### Week 5
- [ ] í•™ìŠµ ì™„ë£Œ
- [ ] ì¶”ë¡  ì¸í„°í˜ì´ìŠ¤ êµ¬í˜„
- [ ] ì„±ëŠ¥ ìµœì í™”
- [ ] ë¬¸ì„œí™” ì™„ë£Œ

---

## ğŸ›  ê°œë°œ í™˜ê²½ ê¶Œì¥ì‚¬í•­

### í•„ìˆ˜ ë„êµ¬
- Python 3.9+
- PyTorch 2.0+
- Git
- VSCode or PyCharm

### ì„ íƒ ë„êµ¬
- Weights & Biases (í•™ìŠµ ëª¨ë‹ˆí„°ë§)
- Jupyter Notebook (ë°ì´í„° íƒìƒ‰)
- tmux (ì¥ì‹œê°„ í•™ìŠµìš©)
- htop (ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§)

---

## ğŸ“š ì°¸ê³  ìë£Œ

### ë…¼ë¬¸
- "Attention Is All You Need" (Transformer)
- "RoFormer: Enhanced Transformer with Rotary Position Embedding"
- "GLU Variants Improve Transformer"
- "GQA: Training Generalized Multi-Query Transformer"

### êµ¬í˜„ ë ˆí¼ëŸ°ìŠ¤
- Hugging Face Transformers
- nanoGPT (Andrej Karpathy)
- LLaMA implementation
- GPT-NeoX

### í•œêµ­ì–´ NLP ë¦¬ì†ŒìŠ¤
- KorQuAD
- Korean Hate Speech Dataset
- AIHub

---

## âš ï¸ ì£¼ì˜ì‚¬í•­ ë° íŒ

### MacBook Air ì œì•½ì‚¬í•­
- **ì—´ ê´€ë¦¬**: ì¥ì‹œê°„ í•™ìŠµ ì‹œ ì¿¨ë§ íŒ¨ë“œ ê¶Œì¥
- **ë°°ì¹˜ í¬ê¸°**: ë©”ëª¨ë¦¬ ì˜¤ë¥˜ ì‹œ ì¤„ì´ê¸°
- **ì „ì› ì—°ê²°**: í•™ìŠµ ì¤‘ ë°˜ë“œì‹œ ì „ì› ì—°ê²°
- **ë°±ê·¸ë¼ìš´ë“œ ì•±**: í•™ìŠµ ì¤‘ ë‹¤ë¥¸ ì•± ìµœì†Œí™”

### ì‹œê°„ ì ˆì•½ íŒ
- ì‘ì€ ëª¨ë¸ë¶€í„° ì‹œì‘ (50M)
- ì „ì²´ íŒŒì´í”„ë¼ì¸ ë¨¼ì € ê²€ì¦
- Pretrained embedding í™œìš© ê³ ë ¤
- í•™ìŠµ ë°ì´í„° ì ì§„ì  ì¦ê°€

### ì¼ë°˜ì  ë¬¸ì œ í•´ê²°
- **OOM Error**: batch_size ê°ì†Œ, gradient_checkpointing í™œì„±í™”
- **Slow Training**: Mixed precision ì‚¬ìš©, ë°ì´í„° ë¡œë”© ìµœì í™”
- **Poor Quality**: ë” ë§ì€ ë°ì´í„°, ë” ê¸´ í•™ìŠµ
- **Divergence**: Learning rate ê°ì†Œ, Gradient clipping

---

## ğŸ“ ë‹¤ìŒ ë‹¨ê³„

1. **GitHub Repository ìƒì„±**
   ```bash
   gh repo create korean-tiny-llm --public
   cd korean-tiny-llm
   ```

2. **ì´ ë¬¸ì„œë¥¼ `docs/roadmap.md`ë¡œ ì €ì¥**

3. **Phase 1 ì‹œì‘**
   - í”„ë¡œì íŠ¸ êµ¬ì¡° ìƒì„±
   - requirements.txt ì‘ì„±
   - ì²« ì»¤ë°‹

ì¤€ë¹„ë˜ë©´ ë³¸ê²©ì ì¸ êµ¬í˜„ì„ ì‹œì‘í•˜ì‹œë©´ ë©ë‹ˆë‹¤!