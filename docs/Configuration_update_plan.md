# ì„¤ì • íŒŒì¼ ì—…ë°ì´íŠ¸ ê³„íš

## ðŸ“‹ ì—…ë°ì´íŠ¸ í•„ìš” íŒŒì¼ ëª©ë¡

### 1. ëª¨ë¸ ì„¤ì • íŒŒì¼

#### ìƒì„±: `configs/model_medium.yaml`
```yaml
# M4 Max ìµœì í™” ì¤‘í˜• ëª¨ë¸ (ê¶Œìž¥)
vocab_size: 32000
hidden_size: 1024
num_layers: 24
num_heads: 16
num_kv_heads: 4  # GQA
intermediate_size: 4096  # SwiGLU (4 * hidden_size)
max_seq_length: 2048
rope_theta: 10000.0
dropout: 0.1

# ì˜ˆìƒ íŒŒë¼ë¯¸í„°: ~350M
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~8-10GB (í•™ìŠµ ì‹œ)
```

#### ìƒì„±: `configs/model_large.yaml`
```yaml
# M4 Max ëŒ€í˜• ëª¨ë¸ (ë„ì „ ê³¼ì œ)
vocab_size: 32000
hidden_size: 1536
num_layers: 24
num_heads: 24
num_kv_heads: 6  # GQA
intermediate_size: 6144  # SwiGLU (4 * hidden_size)
max_seq_length: 2048
rope_theta: 10000.0
dropout: 0.1

# ì˜ˆìƒ íŒŒë¼ë¯¸í„°: ~800M
# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: ~18-22GB (í•™ìŠµ ì‹œ)
```

#### ìœ ì§€: `configs/model_small.yaml`
- ê¸°ì¡´ íŒŒì¼ ìœ ì§€ (í…ŒìŠ¤íŠ¸/ë””ë²„ê¹…ìš©)
- ì£¼ì„ ì¶”ê°€: "ë ˆê±°ì‹œ - í…ŒìŠ¤íŠ¸ìš©"

---

### 2. í•™ìŠµ ì„¤ì • íŒŒì¼

#### ìƒì„±: `configs/training_m4max.yaml`
```yaml
# M4 Max ìµœì í™” í•™ìŠµ ì„¤ì • (Medium ëª¨ë¸ìš©)

seed: 1337
device: mps  # Metal Performance Shaders

data:
  train_path: data/processed/train.txt
  valid_path: data/processed/valid.txt
  tokenizer_model: models/tokenizer.model

train:
  # ë°°ì¹˜ ì„¤ì •
  batch_size: 16
  gradient_accumulation_steps: 4
  effective_batch_size: 64  # 16 * 4
  max_seq_length: 2048
  
  # ì˜µí‹°ë§ˆì´ì €
  learning_rate: 2.0e-4
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  # ìŠ¤ì¼€ì¤„ëŸ¬
  warmup_steps: 4000
  max_steps: 200000
  lr_scheduler: cosine
  min_lr: 2.0e-5  # learning_rateì˜ 10%
  
  # ì²´í¬í¬ì¸íŠ¸
  save_steps: 5000
  eval_steps: 1000
  logging_steps: 100
  output_dir: checkpoints/medium/
  
  # ìµœì í™”
  mixed_precision: true  # FP16
  gradient_checkpointing: false  # ë©”ëª¨ë¦¬ ì¶©ë¶„í•˜ë©´ ë„ê¸°
  compile_model: true  # torch.compile ì‚¬ìš©
  
  # ë°ì´í„° ë¡œë”©
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4

optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

# W&B ë¡œê¹… (ì„ íƒ)
wandb:
  enabled: true
  project: kr-mini-llm
  name: medium-m4max
  tags: [m4max, medium, 350m]
```

#### ìƒì„±: `configs/training_m4max_large.yaml`
```yaml
# M4 Max ìµœì í™” í•™ìŠµ ì„¤ì • (Large ëª¨ë¸ìš©)

seed: 1337
device: mps

data:
  train_path: data/processed/train.txt
  valid_path: data/processed/valid.txt
  tokenizer_model: models/tokenizer.model

train:
  # ë°°ì¹˜ ì„¤ì • (ë©”ëª¨ë¦¬ ì œì•½)
  batch_size: 8
  gradient_accumulation_steps: 8
  effective_batch_size: 64
  max_seq_length: 1536  # 2048ë³´ë‹¤ ì•½ê°„ ìž‘ê²Œ
  
  # ì˜µí‹°ë§ˆì´ì €
  learning_rate: 1.5e-4  # ë” í° ëª¨ë¸ì€ ë” ë‚®ì€ LR
  weight_decay: 0.1
  max_grad_norm: 1.0
  
  # ìŠ¤ì¼€ì¤„ëŸ¬
  warmup_steps: 5000
  max_steps: 200000
  lr_scheduler: cosine
  min_lr: 1.5e-5
  
  # ì²´í¬í¬ì¸íŠ¸
  save_steps: 5000
  eval_steps: 1000
  logging_steps: 100
  output_dir: checkpoints/large/
  
  # ìµœì í™”
  mixed_precision: true
  gradient_checkpointing: true  # ë©”ëª¨ë¦¬ ì ˆì•½ í•„ìš”
  compile_model: true
  
  # ë°ì´í„° ë¡œë”©
  num_workers: 8
  pin_memory: true
  persistent_workers: true
  prefetch_factor: 4

optimizer:
  type: adamw
  betas: [0.9, 0.95]
  eps: 1.0e-8

wandb:
  enabled: true
  project: kr-mini-llm
  name: large-m4max
  tags: [m4max, large, 800m]
```

#### ìˆ˜ì •: `configs/training.yaml`
- ì£¼ì„ ì¶”ê°€: "ë ˆê±°ì‹œ - MacBook Airìš©"
- ìƒˆ íŒŒì¼ ì°¸ì¡° ì¶”ê°€

---

### 3. ì¶”ë¡  ì„¤ì • íŒŒì¼

#### ìƒì„±: `configs/inference.yaml`
```yaml
# ì¶”ë¡  ì„¤ì •

model:
  checkpoint_path: checkpoints/medium/final/
  device: mps
  compile: true  # torch.compileë¡œ ì†ë„ í–¥ìƒ

generation:
  max_length: 512
  temperature: 0.8
  top_p: 0.95
  top_k: 50
  repetition_penalty: 1.1
  do_sample: true
  
  # ë°°ì¹˜ ì¶”ë¡ 
  batch_size: 4
  use_cache: true  # KV cache ì‚¬ìš©

# ì„±ëŠ¥ ëª©í‘œ
# - Medium ëª¨ë¸: > 50 tokens/sec
# - Large ëª¨ë¸: > 30 tokens/sec
```

---

## ðŸ”§ ì½”ë“œ ìˆ˜ì • í•„ìš” ì‚¬í•­

### 1. `src/model/config.py`
```python
@dataclass
class TransformerConfig:
    """M4 Max ìµœì í™” ì„¤ì •"""
    vocab_size: int = 32000
    hidden_size: int = 1024  # 768 â†’ 1024
    num_layers: int = 24     # 12 â†’ 24
    num_heads: int = 16      # 12 â†’ 16
    num_kv_heads: int = 4
    intermediate_size: int = 4096  # 2048 â†’ 4096
    max_seq_length: int = 2048     # 1024 â†’ 2048
    rope_theta: float = 10000.0
    dropout: float = 0.1     # 0.0 â†’ 0.1
    
    @classmethod
    def from_yaml(cls, path: str) -> "TransformerConfig":
        """YAML íŒŒì¼ì—ì„œ ì„¤ì • ë¡œë“œ"""
        import yaml
        with open(path) as f:
            config = yaml.safe_load(f)
        return cls(**config)
    
    @classmethod
    def medium(cls) -> "TransformerConfig":
        """Medium ëª¨ë¸ í”„ë¦¬ì…‹"""
        return cls()
    
    @classmethod
    def large(cls) -> "TransformerConfig":
        """Large ëª¨ë¸ í”„ë¦¬ì…‹"""
        return cls(
            hidden_size=1536,
            num_heads=24,
            num_kv_heads=6,
            intermediate_size=6144
        )
```

### 2. `scripts/train.py` ìˆ˜ì • í•„ìš”
```python
# MPS ë””ë°”ì´ìŠ¤ ì„¤ì •
if config.device == "auto":
    if torch.backends.mps.is_available():
        device = torch.device("mps")
    elif torch.cuda.is_available():
        device = torch.device("cuda")
    else:
        device = torch.device("cpu")
else:
    device = torch.device(config.device)

# torch.compile ì ìš©
if config.train.compile_model:
    model = torch.compile(model, mode="reduce-overhead")

# MPS ë©”ëª¨ë¦¬ ê´€ë¦¬
if device.type == "mps":
    torch.mps.set_per_process_memory_fraction(0.8)
```

### 3. `scripts/generate.py` ìˆ˜ì • í•„ìš”
```python
# KV cache êµ¬í˜„
# ì¶”ë¡  ì†ë„ ìµœì í™”
# ë°°ì¹˜ ì¶”ë¡  ì§€ì›
```

---

## ðŸ“Š ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ ëª©í‘œ

### Medium ëª¨ë¸ (350M)
- **í•™ìŠµ ì†ë„**: 2-3 steps/sec
- **í•™ìŠµ ì‹œê°„**: ~22ì‹œê°„ (200k steps)
- **ì¶”ë¡  ì†ë„**: > 50 tokens/sec
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ~10GB
- **Validation PPL**: < 15

### Large ëª¨ë¸ (800M)
- **í•™ìŠµ ì†ë„**: 1-1.5 steps/sec
- **í•™ìŠµ ì‹œê°„**: ~44ì‹œê°„ (200k steps)
- **ì¶”ë¡  ì†ë„**: > 30 tokens/sec
- **ë©”ëª¨ë¦¬ ì‚¬ìš©**: ~20GB
- **Validation PPL**: < 12

---

## ðŸš€ ì‹¤í–‰ ìˆœì„œ

### Phase 2: ì•„í‚¤í…ì²˜ êµ¬í˜„ (Code ëª¨ë“œ í•„ìš”)
1. ì„¤ì • íŒŒì¼ ìƒì„±
   - `configs/model_medium.yaml`
   - `configs/model_large.yaml`
   - `configs/training_m4max.yaml`
   - `configs/training_m4max_large.yaml`
   - `configs/inference.yaml`

2. ì½”ë“œ ìˆ˜ì •
   - `src/model/config.py` - ì„¤ì • í´ëž˜ìŠ¤ ì—…ë°ì´íŠ¸
   - `scripts/train.py` - MPS ì§€ì› ì¶”ê°€
   - `scripts/generate.py` - ì¶”ë¡  ìµœì í™”

3. í…ŒìŠ¤íŠ¸
   - ì„¤ì • íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
   - ë”ë¯¸ ë°ì´í„°ë¡œ forward pass í…ŒìŠ¤íŠ¸
   - ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

### Phase 3: ë°ì´í„° ì¤€ë¹„
1. í•œêµ­ì–´ ë°ì´í„° ìˆ˜ì§‘ (20-50GB)
2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
3. Tokenizer í•™ìŠµ (32k vocab)

### Phase 4: í•™ìŠµ
1. Medium ëª¨ë¸ í•™ìŠµ ì‹œìž‘
2. ëª¨ë‹ˆí„°ë§ ë° ì¡°ì •
3. ì²´í¬í¬ì¸íŠ¸ í‰ê°€

---

## âœ… ì²´í¬ë¦¬ìŠ¤íŠ¸

### ì„¤ì • íŒŒì¼ ìƒì„±
- [ ] `configs/model_medium.yaml`
- [ ] `configs/model_large.yaml`
- [ ] `configs/training_m4max.yaml`
- [ ] `configs/training_m4max_large.yaml`
- [ ] `configs/inference.yaml`

### ì½”ë“œ ìˆ˜ì •
- [ ] `src/model/config.py` - í”„ë¦¬ì…‹ ë©”ì„œë“œ ì¶”ê°€
- [ ] `scripts/train.py` - MPS ì§€ì›
- [ ] `scripts/generate.py` - ì¶”ë¡  ìµœì í™”
- [ ] `src/training/trainer.py` - í•™ìŠµ ë£¨í”„ êµ¬í˜„

### í…ŒìŠ¤íŠ¸
- [ ] ì„¤ì • íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
- [ ] ëª¨ë¸ ì´ˆê¸°í™” í…ŒìŠ¤íŠ¸
- [ ] Forward/Backward pass í…ŒìŠ¤íŠ¸
- [ ] ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

---

## ðŸŽ¯ ë‹¤ìŒ ë‹¨ê³„

**Plan ëª¨ë“œì—ì„œ ì™„ë£Œ**:
- âœ… M4 Max ìµœì í™” ê³„íš ìˆ˜ë¦½
- âœ… ë¬¸ì„œ ì—…ë°ì´íŠ¸
- âœ… ì„¤ì • íŒŒì¼ ê³„íš ìž‘ì„±

**Code ëª¨ë“œë¡œ ì „í™˜ í•„ìš”**:
- ì‹¤ì œ ì„¤ì • íŒŒì¼ ìƒì„± (YAML)
- ì½”ë“œ ìˆ˜ì • (Python)
- í…ŒìŠ¤íŠ¸ ì‹¤í–‰

Code ëª¨ë“œë¡œ ì „í™˜í•˜ì—¬ ì‹¤ì œ íŒŒì¼ì„ ìƒì„±í•˜ê³  ì½”ë“œë¥¼ ìˆ˜ì •í•˜ì‹œê² ìŠµë‹ˆê¹Œ?